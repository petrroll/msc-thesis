\chapter*{Conclusion}\label{ch:6}
\addcontentsline{toc}{chapter}{Conclusion}
The incorporation of biological detail into deep neural architectures for the purpose of understanding the brain computation is a promising but still only a poorly explored area of computational neuroscience. The HSM model introduced by \cite{antolik}, that was the focus of this thesis, is one such example. While, at the time of writing, outperforming state-of-the-art approaches, the HSM model was implemented in a very ad-hoc fashion, relatively sensitive to initial conditions, and poorly characterized and compared with respect to similar but more traditional DNN architectures. These shortcomings posed a significant barrier for future development of this bio-inspired architecture. The main goal of this thesis was to remedy this situation. 

This thesis offers following contributions:

\begin{itemize}
    \item Reimplementation of the model in modern neuroscience oriented DNN\\ framework.
    \item Identification of hyper-parametrization that improves modelâ€™s robustness with respect to initial conditions and input datasets.
    \item Comparison of HSM architecture to more traditional DNNs, showing its advantage - especially especially with respect to robustness.
    \item Evaluation of dataset pooling and model robustness with respect to different regions recording both within the same and across different animals.
\end{itemize}

\section*{Main findings}
\addcontentsline{toc}{section}{Main findings}

The first set of experiments showed us that non-architectural hyperparameters such as learning rate and the scale of input data can have a big impact. Similarly, when those are already tuned, the gains achievable by regularization are relatively minimal - at least for the HSM architecture featuring a plain DoG layer with its hard regularization in the form of explicitly parameterized filters. The second set went further, showing that when properly regularized and fine-tuned, various types of more generic filters (convolution, convolutional DoG, fully connected) work almost identically when used as the first layer instead of DoG layer. Similarly, it failed to show a substantial improvement for variations with a separable hidden layer instead of a fully connected one. It also revealed unexpected interactions with respect to a non-linearity after the first filter layer of the model. 

The third section gave us two insights. First, even though the regions should be similar, improvements made on one do not trivially translate to others. On region 3 we observed a variant of our HSM-based architecture fine-tuned on region 1 perform worse than less tuned versions. Further, we found evidence that even though more generic variants can achieve a similar level of performance as the HSM architecture - with its plain DoG layer, when carefully optimized, they are significantly less robust when used on other regions without hyperparameter changes. Second, pooling regions together and training one shared model does not bring expected benefits, and can actually be detrimental. At least for a variation where all three regions are combined without further processing.

\section*{Lessons learned}
\addcontentsline{toc}{section}{Lessons learned}

Putting it all together, few themes emerge. First, small architectural variations of computationally very similar building blocks are not as significant when best achievable performance is concerned. The biggest difference is in the need for well-tuned regularizations. These are paramount for architectures with higher amounts of free parameters, such as with unrestricted convolution layers or the LNLN variant. With regularization and proper hyperparameters, training to a comparable level of performance similarly quickly is possible, however. 

The benefit of more constrained versions, such as the plain DoG layer variant, is mainly when exhaustive grid search across - not only regularization - hyperparameters is not an option. For example, when we want to use a model fine-tuned on one region on another without further modifications. On this note, the comparable but not better performance of more generic variants of first layer filters (LNLN, convolution) serve as further evidence that the initial part of visual computation might be well approximated using difference-of-Gaussians filters.

Second, the variability of performance with respect to small hyperparameters changes both within a region but mainly across regions can be significant, especially for certain architectures, and the space of all potential hyperparameter combinations is vast. Thus, any conclusions about the underlying fundamentals of the biological computation, that we are trying to model, should be done very carefully. So as not to mistake an observation of one model with a particular set of hyperparameters being better than another one with its set of hyperparameters for a signal about the biological principles of the brain. Similarly, training a set of models per each model instance and analysing distributions of runs instead of single runs proved to be a necessity to even attempt to draw any statistically significant conclusions.

Third, basic deep learning intuition trained mainly on a few types of computer vision problems with vast not-noisy datasets can be helpful but should never be blindly relied upon. As we have seen with the learning rate experiment, explorations of the input data scaling, or additional non-linearity tests, changes that in more standard ML settings might have smaller impact can be relatively influential with this type of data. This ties back to our second lesson learned. Even a significant difference in performance across multiple instantiations of the same model can be due to, for example, the optimizer interacting with a particular data scale better on one architecture than on another and have nothing to do with the similarity between the individual architectures and the computation they are trying to model.

\section*{Future work}
\addcontentsline{toc}{section}{Future work}

As already discussed at various places throughout the \nameref{ch:5} chapter, there are plenty of opportunities for further research. We identified four main areas: further investigation of bigger and computationally non-equivalent architectural changes, analysis of layers activations and the impact of additional non-linearities, comparison of data between regions and proper analysis of models trained on pooled data, and a more involved investigation of the trained weights of both the first layer filters and second layer masks for various architectures already explored in this work.

For further architectures, several options are readily available. Proper reimplementation of the what/where model as introduced by \citeauthor{klindt} could be a great starting point that could be followed by exploration of novel deeper architectures, possibly mixing generic convolutions with convolutional and plain DoG layers. Further, we suggest incorporating other layer types in new combinations, such as rotational-equivariant convolutional filters \citep{ecker} or, in NDN3 already implemented, separable versions of normal convolutions. Additionally, variants of the HSM architecture with only a single readout layer that is not followed by a fully connected output layer or deeper architectures with less non-linearities could be explored. We also suggest continuing investigating transfer learning with classical computer vision datasets, especially for the initial filter layers. 

Adding batch normalization\footnote{\citep{2015arXiv150203167I}} might also answer some questions - especially around the input scale impact, and potentially unblock deeper architectures. As already discussed in the Experiments and results section, proper investigation into why and how the additional non-linearity on the first filter layer impacts performance could also lead to informative conclusions.

For the third area, we suggest looking at the individual region datasets statistically, and comparing whether they are, in fact, similar in terms of distribution, within region cross-correlations, etc. On the model side, an analysis of what architectures and hyperparameters work on each region, similar to our work on region 1, could also shed some light on the differences between regions. It might also unblock the potential that, we still believe, lies in pooling the three regions and training just one shared model, essentially leveraging within dataset transfer learning. On that note, a good starting point on that might be to test why the combined model had worse performance than the ensemble. Whether it was due to worse performance on only one region or if all regions neurons were negatively impacted by the shared training.

Lastly, an exhaustive comparison of the trained first layer filters and second layer masks of the architectures explored in this thesis could help to unravel the impacts of hard versus soft regularizations. It could answer the question of whether the other first layer variants - that are essentially just more relaxed versions of the DoG layer - converge to it, or if they just achieve a similar level of performance with substantially different filters. Similar inquiry could be done for the hidden readout layer. As part of this, an investigation into how the filters change during training and what is the role of random initialization might also be worth pursuing.
