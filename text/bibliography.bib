%
%  An example of a bibliographical database for bibTeX
%
%  Recommended software for maintenance of *.bib files:
%    JabRef, http://jabref.sourceforge.net/
%
%  BEWARE:
%
%    *  If a name contains a capital letter, which must be kept such,
%       use curly brackets ({T}hailand, {HIV}).
%
%  ===========================================================================

@Book{schwartz1990computational,
 author = {Schwartz, Eric},
 title = {Computational neuroscience},
 publisher = {MIT Press},
 year = {1990},
 address = {Cambridge, Mass},
 isbn = {978-0-262-19291-0}
 }
 
 @ARTICLE{Lapicque1907,
  author = {L. F. Abott},
  title = {Lapicque’s introduction of theintegrate-and-fire model neuron (1907)},
  journal = {Brain Research Bulletin},
  year = {1999},
  volume = {50},
  pages = {303--304},
}

@article{Lindsay_2020,
   title={Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future},
   ISSN={1530-8898},
   url={http://dx.doi.org/10.1162/jocn_a_01544},
   DOI={10.1162/jocn_a_01544},
   journal={Journal of Cognitive Neuroscience},
   publisher={MIT Press - Journals},
   author={Lindsay, Grace W.},
   year={2020},
   month={Feb},
   pages={1–15}
}

@article{antolik,
    author = {Antolík, Ján AND Hofer, Sonja B. AND Bednar, James A. AND Mrsic-Flogel, Thomas D.},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Model Constrained by Visual Hierarchy Improves Prediction of Neural Responses to Natural Scenes},
    year = {2016},
    month = {06},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pcbi.1004927},
    pages = {1-22},
    abstract = {Author Summary A key goal in sensory neuroscience is to understand the relationship between sensory stimuli and patterns of activity they elicit in networks of sensory neurons. Many models have been proposed in the past; however, these models have largely ignored the known architecture of primary visual cortex revealed in experimental studies, thus limiting their ability to accurately describe neural responses to sensory stimuli. Here we propose a model of primary visual cortex that takes into account the known architecture of visual cortex, specifically the fact that only a limited number of thalamic inputs with stereotypical receptive fields are shared within a local area of visual cortex, and the hierarchical progression from neurons with linear receptive fields (simple cells) to neurons with non-linear receptive fields (complex cells). We show that the proposed model outperforms state-of-the-art methods for receptive field estimation when fitted to two-photon calcium recordings of local populations of mouse V1 neurons responding to natural image stimuli. The model demonstrates how the diverse set of receptive fields in the local population of neurons can be constructed from a limited number (< 20) thalamic inputs.},
    number = {6},
    doi = {10.1371/journal.pcbi.1004927}
}

@book{bear_neuroscience:_2007,
	address = {Philadelphia, PA},
	edition = {3rd ed},
	title = {Neuroscience: exploring the brain},
	isbn = {9780781776073 9780781760034},
	shorttitle = {Neuroscience},
	publisher = {Lippincott Williams \& Wilkins},
	author = {Bear, Mark F. and Connors, Barry W. and Paradiso, Michael A.},
	year = {2007},
	note = {OCLC: ocm62509134},
	keywords = {Neurosciences, Brain, Brain, Neurosciences, Spinal Cord},
}

@article {Carandini10577,
	author = {Carandini, Matteo and Demb, Jonathan B. and Mante, Valerio and Tolhurst, David J. and Dan, Yang and Olshausen, Bruno A. and Gallant, Jack L. and Rust, Nicole C.},
	title = {Do We Know What the Early Visual System Does?},
	volume = {25},
	number = {46},
	pages = {10577--10597},
	year = {2005},
	doi = {10.1523/JNEUROSCI.3726-05.2005},
	publisher = {Society for Neuroscience},
	abstract = {We can claim that we know what the visual system does once we can predict neural responses to arbitrary stimuli, including those seen in nature. In the early visual system, models based on one or more linear receptive fields hold promise to achieve this goal as long as the models include nonlinear mechanisms that control responsiveness, based on stimulus context and history, and take into account the nonlinearity of spike generation. These linear and nonlinear mechanisms might be the only essential determinants of the response, or alternatively, there may be additional fundamental determinants yet to be identified. Research is progressing with the goals of defining a single {\textquotedblleft}standard model{\textquotedblright} for each stage of the visual pathway and testing the predictive power of these models on the responses to movies of natural scenes. These predictive models represent, at a given stage of the visual pathway, a compact description of visual computation. They would be an invaluable guide for understanding the underlying biophysical and anatomical mechanisms and relating neural responses to visual perception.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/25/46/10577},
	eprint = {https://www.jneurosci.org/content/25/46/10577.full.pdf},
	journal = {Journal of Neuroscience}
}

@phdthesis{thesis_2015,
  title = "Systematising glyph design for visualization",
  author = "Eamonn Maguire",
  year = "2015",
  affiliation = "Oxford University",
  institution = "Oxford University",
  school = "Oxford University",
  journal = "DPhil Thesis",
  url = "http://ora.ox.ac.uk/objects/uuid:b98ccce1-038f-4c0a-a259-7f53dfe06ac7",
}

@misc{pict_gabor_filter,
 author  = "Joe Pharos",
 title   = "Gabor filter",
 year    = "2006",
 urlseen = "17-11-20",
 url     = "https://commons.wikimedia.org/wiki/File:Gabor_filter.png",
}

@article{doi:10.1146/annurev-vision-091718-014731,
author = {Butts, Daniel A.},
title = {Data-Driven Approaches to Understanding Visual Neuron Activity},
journal = {Annual Review of Vision Science},
volume = {5},
number = {1},
pages = {451-477},
year = {2019},
doi = {10.1146/annurev-vision-091718-014731},
    note ={PMID: 31386605},

URL = { 
        https://doi.org/10.1146/annurev-vision-091718-014731
    
},
eprint = { 
        https://doi.org/10.1146/annurev-vision-091718-014731
    
}
,
    abstract = { With modern neurophysiological methods able to record neural activity throughout the visual pathway in the context of arbitrarily complex visual stimulation, our understanding of visual system function is becoming limited by the available models of visual neurons that can be directly related to such data. Different forms of statistical models are now being used to probe the cellular and circuit mechanisms shaping neural activity, understand how neural selectivity to complex visual features is computed, and derive the ways in which neurons contribute to systems-level visual processing. However, models that are able to more accurately reproduce observed neural activity often defy simple interpretations. As a result, rather than being used solely to connect with existing theories of visual processing, statistical modeling will increasingly drive the evolution of more sophisticated theories. }
}

@article{first_kernel,
author = {Willmore, Ben and Smyth, Darragh},
year = {2003},
month = {09},
pages = {553-77},
title = {Methods for first-order kernel estimation: Simple-cell receptive fields from responses to natural scenes},
volume = {14},
journal = {Network (Bristol, England)},
doi = {10.1088/0954-898X/14/3/309}
}

@Article{Kay2008,
author={Kay, Kendrick N.
and Naselaris, Thomas
and Prenger, Ryan J.
and Gallant, Jack L.},
title={Identifying natural images from human brain activity},
journal={Nature},
year={2008},
month={Mar},
day={01},
volume={452},
number={7185},
pages={352-355},
abstract={Recent functional magnetic resonance imaging (fMRI) studies have shown that, based on patterns of activity evoked by different categories of visual images, it is possible to deduce simple features in the visual scene, or to which category it belongs. Kay et al. take this approach a tantalizing step further. Their newly developed decoding method, based on quantitative receptive field models that characterize the relationship between visual stimuli and fMRI activity in early visual areas, can identify with high accuracy which specific natural image an observer saw, even for an image chosen at random from 1,000 distinct images. This prompts the thought that it may soon be possible to decode subjective perceptual experiences such as visual imagery and dreams, an idea previously restricted to the realm of science fiction.},
issn={1476-4687},
doi={10.1038/nature06713},
url={https://doi.org/10.1038/nature06713}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{Rosenblatt1958ThePA,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={F. Rosenblatt},
  journal={Psychological review},
  year={1958},
  volume={65 6},
  pages={
          386-408
        }
}

@mastersthesis{thesis_hojdar,
  title = "Using neural networks to generate realistic skies",
  author = "Štěpán Hojdar",
  year = "2019",
  school = "Charles University",
  url = "https://is.cuni.cz/webapps/zzp/detail/213909/",
}

@article{kiefer1952,
author = "Kiefer, J. and Wolfowitz, J.",
doi = "10.1214/aoms/1177729392",
fjournal = "Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "09",
number = "3",
pages = "462--466",
publisher = "The Institute of Mathematical Statistics",
title = "Stochastic Estimation of the Maximum of a Regression Function",
url = "https://doi.org/10.1214/aoms/1177729392",
volume = "23",
year = "1952"
}

@ARTICLE{kingma2014adam,
       author = {{Kingma}, Diederik P. and {Ba}, Jimmy},
        title = "{Adam: A Method for Stochastic Optimization}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2014,
        month = dec,
          eid = {arXiv:1412.6980},
        pages = {arXiv:1412.6980},
archivePrefix = {arXiv},
       eprint = {1412.6980},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6980K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{JMLR:v15:srivastava14a,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@ARTICLE{2015arXiv150203167I,
       author = {{Ioffe}, Sergey and {Szegedy}, Christian},
        title = "{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2015,
        month = feb,
          eid = {arXiv:1502.03167},
        pages = {arXiv:1502.03167},
archivePrefix = {arXiv},
       eprint = {1502.03167},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv150203167I},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180801974T,
       author = {{Tan}, Chuanqi and {Sun}, Fuchun and {Kong}, Tao and {Zhang}, Wenchang and
         {Yang}, Chao and {Liu}, Chunfang},
        title = "{A Survey on Deep Transfer Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2018,
        month = aug,
          eid = {arXiv:1808.01974},
        pages = {arXiv:1808.01974},
archivePrefix = {arXiv},
       eprint = {1808.01974},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180801974T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{cifar10,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {2015},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
}

@misc{xiao2017,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

@ARTICLE{2018arXiv180402767R,
       author = {{Redmon}, Joseph and {Farhadi}, Ali},
        title = "{YOLOv3: An Incremental Improvement}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2018,
        month = apr,
          eid = {arXiv:1804.02767},
        pages = {arXiv:1804.02767},
archivePrefix = {arXiv},
       eprint = {1804.02767},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180402767R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{klidnt,
author = {Klindt, David A. and Ecker, Alexander S. and Euler, Thomas and Bethge, Matthias},
title = {Neural System Identification for Large Populations Separating What and Where},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Neuroscientists classify neurons into different types that perform similar computations at different locations in the visual field. Traditional methods for neural system identification do not capitalize on this separation of "what" and "where". Learning deep convolutional feature spaces that are shared among many neurons provides an exciting path forward, but the architectural design needs to account for data limitations: While new experimental techniques enable recordings from thousands of neurons, experimental time is limited so that one can sample only a small fraction of each neuron's response space. Here, we show that a major bottleneck for fitting convolutional neural networks (CNNs) to neural data is the estimation of the individual receptive field locations - a problem that has been scratched only at the surface thus far. We propose a CNN architecture with a sparse readout layer factorizing the spatial (where) and feature (what) dimensions. Our network scales well to thousands of neurons and short recordings and can be trained end-to-end. We evaluate this architecture on ground-truth data to explore the challenges and limitations of CNN-based system identification. Moreover, we show that our network model outperforms current state-of-the art system identification models of mouse primary visual cortex.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {3509–3519},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@ARTICLE{2019arXiv190605909R,
       author = {{Ramachandran}, Prajit and {Parmar}, Niki and {Vaswani}, Ashish and
         {Bello}, Irwan and {Levskaya}, Anselm and {Shlens}, Jonathon},
        title = "{Stand-Alone Self-Attention in Vision Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2019,
        month = jun,
          eid = {arXiv:1906.05909},
        pages = {arXiv:1906.05909},
archivePrefix = {arXiv},
       eprint = {1906.05909},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190605909R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019arXiv190409925B,
       author = {{Bello}, Irwan and {Zoph}, Barret and {Vaswani}, Ashish and
         {Shlens}, Jonathon and {Le}, Quoc V.},
        title = "{Attention Augmented Convolutional Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2019,
        month = apr,
          eid = {arXiv:1904.09925},
        pages = {arXiv:1904.09925},
archivePrefix = {arXiv},
       eprint = {1904.09925},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190409925B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{dosovitskiy2020image,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    year={2020},
    eid = {arXiv:2010.11929},
    pages = {arXiv:2010.11929},
    eprint = {2010.11929},
    primaryClass = {cs.CV},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    journal = {arXiv e-prints},
}

@ARTICLE{ecker,
       author = {{Ecker}, Alexander S. and {Sinz}, Fabian H. and
         {Froudarakis}, Emmanouil and {Fahey}, Paul G. and
         {Cadena}, Santiago A. and {Walker}, Edgar Y. and {Cobos}, Erick and
         {Reimer}, Jacob and {Tolias}, Andreas S. and {Bethge}, Matthias},
        title = "{A rotation-equivariant convolutional neural network model of primary visual cortex}",
      journal = {arXiv e-prints},
     keywords = {Quantitative Biology - Neurons and Cognition, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2018,
        month = sep,
          eid = {arXiv:1809.10504},
        pages = {arXiv:1809.10504},
archivePrefix = {arXiv},
       eprint = {1809.10504},
 primaryClass = {q-bio.NC},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180910504E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016arXiv160207576C,
       author = {{Cohen}, Taco S. and {Welling}, Max},
        title = "{Group Equivariant Convolutional Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2016,
        month = feb,
          eid = {arXiv:1602.07576},
        pages = {arXiv:1602.07576},
archivePrefix = {arXiv},
       eprint = {1602.07576},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160207576C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article {Walke506956,
	author = {Walke, Edgar Y. and Sinz, Fabian H. and Froudarakis, Emmanouil and Fahey, Paul G. and Muhammad, Taliah and Ecker, Alexander S. and Cobos, Erick and Reimer, Jacob and Pitkow, Xaq and Tolias, Andreas S.},
	title = {Inception in visual cortex: in vivo-silico loops reveal most exciting images},
	elocation-id = {506956},
	year = {2018},
	doi = {10.1101/506956},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Much of our knowledge about sensory processing in the brain is based on quasi-linear models and the stimuli that optimally drive them. However, sensory information processing is nonlinear, even in primary sensory areas, and optimizing sensory input is difficult due to the high-dimensional input space. We developed inception loops, a closed-loop experimental paradigm that combines in vivo recordings with in silico nonlinear response modeling to identify the Most Exciting Images (MEIs) for neurons in mouse V1. When presented back to the brain, MEIs indeed drove their target cells significantly better than the best stimuli identified by linear models. The MEIs exhibited complex spatial features that deviated from the textbook ideal of V1 as a bank of Gabor filters. Inception loops represent a widely applicable new approach to dissect the neural mechanisms of sensation.},
	URL = {https://www.biorxiv.org/content/early/2018/12/28/506956},
	eprint = {https://www.biorxiv.org/content/early/2018/12/28/506956.full.pdf},
	journal = {bioRxiv}
}

@article{10.1371/journal.pcbi.1006897,
    author = {Cadena, Santiago A. AND Denfield, George H. AND Walker, Edgar Y. AND Gatys, Leon A. AND Tolias, Andreas S. AND Bethge, Matthias AND Ecker, Alexander S.},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Deep convolutional models improve predictions of macaque V1 responses to natural images},
    year = {2019},
    month = {04},
    volume = {15},
    url = {https://doi.org/10.1371/journal.pcbi.1006897},
    pages = {1-27},
    abstract = {Author summary Predicting the responses of sensory neurons to arbitrary natural stimuli is of major importance for understanding their function. Arguably the most studied cortical area is primary visual cortex (V1), where many models have been developed to explain its function. However, the most successful models built on neurophysiologists’ intuitions still fail to account for spiking responses to natural images. Here, we model spiking activity in primary visual cortex (V1) of monkeys using deep convolutional neural networks (CNNs), which have been successful in computer vision. We both trained CNNs directly to fit the data, and used CNNs trained to solve a high-level task (object categorization). With these approaches, we are able to outperform previous models and improve the state of the art in predicting the responses of early visual neurons to natural images. Our results have two important implications. First, since V1 is the result of several nonlinear stages, it should be modeled as such. Second, functional models of entire visual pathways, of which V1 is an early stage, do not only account for higher areas of such pathways, but also provide useful representations for V1 predictions.},
    number = {4},
    doi = {10.1371/journal.pcbi.1006897}
}

@ARTICLE{VGG16,
       author = {{Simonyan}, Karen and {Zisserman}, Andrew},
        title = "{Very Deep Convolutional Networks for Large-Scale Image Recognition}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2014,
        month = sep,
          eid = {arXiv:1409.1556},
        pages = {arXiv:1409.1556},
archivePrefix = {arXiv},
       eprint = {1409.1556},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1409.1556S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017arXiv170610239W,
       author = {{Wu}, Lei and {Zhu}, Zhanxing and {E}, Weinan},
        title = "{Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
         year = 2017,
        month = jun,
          eid = {arXiv:1706.10239},
        pages = {arXiv:1706.10239},
archivePrefix = {arXiv},
       eprint = {1706.10239},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170610239W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017arXiv170300548M,
       author = {{Miikkulainen}, Risto and {Liang}, Jason and {Meyerson}, Elliot and
         {Rawal}, Aditya and {Fink}, Dan and {Francon}, Olivier and
         {Raju}, Bala and {Shahrzad}, Hormoz and {Navruzyan}, Arshak and
         {Duffy}, Nigel and {Hodjat}, Babak},
        title = "{Evolving Deep Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
         year = 2017,
        month = mar,
          eid = {arXiv:1703.00548},
        pages = {arXiv:1703.00548},
archivePrefix = {arXiv},
       eprint = {1703.00548},
 primaryClass = {cs.NE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170300548M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@mastersthesis{thesis_arnold,
  title = "Bayesian Optimization of Hyperparameters Using Gaussian Processes",
  author = "Jakub Arnold",
  year = "2019",
  school = "Charles University",
  url = "https://is.cuni.cz/webapps/zzp/detail/212253/",
}

@ARTICLE{2016arXiv161101578Z,
       author = {{Zoph}, Barret and {Le}, Quoc V.},
        title = "{Neural Architecture Search with Reinforcement Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
         year = 2016,
        month = nov,
          eid = {arXiv:1611.01578},
        pages = {arXiv:1611.01578},
archivePrefix = {arXiv},
       eprint = {1611.01578},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161101578Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@Article{Goris2014,
author={Goris, Robbe L. T.
and Movshon, J. Anthony
and Simoncelli, Eero P.},
title={Partitioning neuronal variability},
journal={Nature Neuroscience},
year={2014},
month={Jun},
day={01},
volume={17},
number={6},
pages={858-865},
abstract={The authors developed a model of neuron firing in which spike generation arises from the combination of sensory drive and stimulus-independent modulatory influences. This model provides an accurate account of neuron responses in multiple visual areas, suggesting that variability originates from excitability fluctuations that increase in strength along the visual pathway.},
issn={1546-1726},
doi={10.1038/nn.3711},
url={https://doi.org/10.1038/nn.3711}
}

@ARTICLE{2018arXiv180910504E,
       author = {{Ecker}, Alexander S. and {Sinz}, Fabian H. and
         {Froudarakis}, Emmanouil and {Fahey}, Paul G. and
         {Cadena}, Santiago A. and {Walker}, Edgar Y. and {Cobos}, Erick and
         {Reimer}, Jacob and {Tolias}, Andreas S. and {Bethge}, Matthias},
        title = "{A rotation-equivariant convolutional neural network model of primary visual cortex}",
      journal = {arXiv e-prints},
     keywords = {Quantitative Biology - Neurons and Cognition, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2018,
        month = sep,
          eid = {arXiv:1809.10504},
        pages = {arXiv:1809.10504},
archivePrefix = {arXiv},
       eprint = {1809.10504},
 primaryClass = {q-bio.NC},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180910504E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


